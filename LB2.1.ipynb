{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a51b67-344a-4218-87a7-08b063cff2b0",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 6\n",
      "[('обожнюю', 2), ('їсти', 3), ('мені', 3), ('подобається', 1), ('пробувати', 1), ('нові', 1), ('страви', 1), ('та', 1), ('кухні', 1), ('вчора', 1)]\n",
      "Обожнюю їсти! Мені подобається пробувати нові страви та кухні.\n",
      "[[ 0  5  2  3 10 11 12 13 14 15]\n",
      " [ 0 16  6 17 18  4 19 20 21 22]\n",
      " [ 0  0  0  5 23  4 24  2 25 26]\n",
      " [ 0 27 28 29  7 30  1 31  1  8]\n",
      " [ 6 32  4  3  9 33 34  1 35 36]\n",
      " [39 40 41 42  7  3  9  2  1  8]]\n",
      "[('в', 1), ('їсти', 2), ('мені', 3), ('і', 4), ('обожнюю', 5), ('я', 6), ('тому', 7), ('ресторанах', 8), ('складно', 9), ('подобається', 10), ('пробувати', 11), ('нові', 12), ('страви', 13), ('та', 14), ('кухні', 15), ('вчора', 16), ('приготував', 17), ('лазанью', 18), ('вона', 19), ('вийшла', 20), ('дуже', 21), ('смачною', 22), ('піцу', 23), ('можу', 24), ('її', 25), ('щодня', 26), ('не', 27), ('люблю', 28), ('готувати', 29), ('харчуюся', 30), ('основному', 31), ('вегетаріанець', 32), ('знайти', 33), ('їжу', 34), ('деяких', 35), ('країнах', 36), ('у', 37), ('мене', 38), ('алергія', 39), ('на', 40), ('багато', 41), ('продуктів', 42)]\n",
      "(6, 10) (6, 2)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 10, 256)           256000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10, 128)           197120    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 10, 128)           131584    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 634242 (2.42 MB)\n",
      "Trainable params: 634242 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6924 - accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6857 - accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6770 - accuracy: 0.8333\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6611 - accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6280 - accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5535 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3984 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1822 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0655 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0015 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Задані тексти\n",
    "texts_true = [\n",
    "    \"Обожнюю їсти! Мені подобається пробувати нові страви та кухні.\",\n",
    "    \"Вчора я приготував лазанью, і вона вийшла дуже смачною.\",\n",
    "    \"Обожнюю піцу і можу їсти її щодня.\"\n",
    "]\n",
    "\n",
    "texts_false = [\n",
    "    \"Не люблю готувати, тому харчуюся в основному в ресторанах.\",\n",
    "    \"Я вегетаріанець, і мені складно знайти їжу в деяких країнах.\",\n",
    "    \"У мене алергія на багато продуктів, тому мені складно їсти в ресторанах.\"\n",
    "]\n",
    "\n",
    "# Об'єднання текстів\n",
    "texts = texts_true + texts_false\n",
    "count_true = len(texts_true)\n",
    "count_false = len(texts_false)\n",
    "total_lines = count_true + count_false\n",
    "print(count_true, count_false, total_lines)\n",
    "\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»', lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "print(texts[0][:100])\n",
    "\n",
    "max_text_len = 10\n",
    "data = tokenizer.texts_to_sequences(texts)\n",
    "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
    "print(data_pad)\n",
    "\n",
    "print(list(tokenizer.word_index.items()))\n",
    "\n",
    "X = data_pad\n",
    "Y = np.array([[1, 0]]*count_true + [[0, 1]]*count_false)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "indices = np.random.choice(X.shape[0], size=X.shape[0], replace=False)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxWordsCount, 256, input_length=max_text_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=RMSprop(lr=0.00005))\n",
    "\n",
    "history = model.fit(X, Y, batch_size=64, epochs=50)\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "def sequence_to_text(list_of_indices):\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560efc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['я', 'люблю', 'їсти', 'їжу']\n",
      "1/1 [==============================] - 1s 977ms/step\n",
      "[[0.97529864 0.0437435 ]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "t = \"Я люблю їсти смачну їжу.\".lower()\n",
    "data = tokenizer.texts_to_sequences([t])\n",
    "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
    "print(sequence_to_text(data[0]))\n",
    "\n",
    "res = model.predict(data_pad)\n",
    "print(res, np.argmax(res), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6605484-8126-42ec-a1f0-93b5e55bed38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат. Текст позитивний.\n"
     ]
    }
   ],
   "source": [
    "# Визначення результату\n",
    "if np.argmax(res) == 0:\n",
    "    print(\"Результат. Текст позитивний.\")\n",
    "else:\n",
    "    print(\"Результат. Текст негативний.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc1cf87-d509-4860-a2d3-77da92c5aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['я', 'не', 'в', 'мене', 'алергія', 'на']\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "t1 = \"Я не їм горіхи, бо в мене алергія на них.\".lower()\n",
    "data1 = tokenizer.texts_to_sequences([t1])\n",
    "data1_pad = pad_sequences(data1, maxlen=max_text_len)\n",
    "print( sequence_to_text(data1[0]) )\n",
    "\n",
    "res1 = model.predict(data1_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c4b7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат. Текст негативний.\n"
     ]
    }
   ],
   "source": [
    "# Визначення результату\n",
    "if np.argmax(res1) == 0:\n",
    "    print(\"Результат. Текст негативний.\")\n",
    "else:\n",
    "    print(\"Результат. Текст позитивний.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077604f7",
   "metadata": {},
   "source": [
    "Висновок: на лабораторній роботі я навчився створювати RNN для семантичного аналізу з використанням моделі LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
